{
  "papers": [
    {
      "id": "paper_1730783000000_abc123def",
      "title": "Attention Is All You Need",
      "authors": [
        {"name": "Ashish Vaswani"},
        {"name": "Noam Shazeer"},
        {"name": "Niki Parmar"},
        {"name": "Jakob Uszkoreit"},
        {"name": "Llion Jones"},
        {"name": "Aidan N. Gomez"},
        {"name": "≈Åukasz Kaiser"},
        {"name": "Illia Polosukhin"}
      ],
      "year": 2017,
      "url": "https://arxiv.org/abs/1706.03762",
      "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.",
      "source": "crawler",
      "crawledAt": "2024-11-05T05:43:20.000Z",
      "tags": ["transformers", "attention", "neural networks"],
      "dateAdded": "2024-11-05T05:43:20.000Z"
    },
    {
      "id": "paper_1730783000001_def456ghi",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        {"name": "Jacob Devlin"},
        {"name": "Ming-Wei Chang"},
        {"name": "Kenton Lee"},
        {"name": "Kristina Toutanova"}
      ],
      "year": 2018,
      "url": "https://arxiv.org/abs/1810.04805",
      "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.",
      "source": "crawler",
      "crawledAt": "2024-11-05T05:43:20.000Z",
      "tags": ["bert", "transformers", "language understanding", "nlp"],
      "dateAdded": "2024-11-05T05:43:20.000Z"
    },
    {
      "id": "paper_1730783000002_ghi789jkl",
      "title": "GPT-3: Language Models are Few-Shot Learners",
      "authors": [
        {"name": "Tom B. Brown"},
        {"name": "Benjamin Mann"},
        {"name": "Nick Ryder"},
        {"name": "Melanie Subbiah"},
        {"name": "Jared Kaplan"},
        {"name": "Prafulla Dhariwal"},
        {"name": "Arvind Neelakantan"},
        {"name": "Pranav Shyam"},
        {"name": "Girish Sastry"},
        {"name": "Amanda Askell"},
        {"name": "Sandhini Agarwal"},
        {"name": "Ariel Herbert-Voss"},
        {"name": "Gretchen Krueger"},
        {"name": "Tom Henighan"},
        {"name": "Rewon Child"},
        {"name": "Aditya Ramesh"},
        {"name": "Daniel M. Ziegler"},
        {"name": "Jeffrey Wu"},
        {"name": "Clemens Winter"},
        {"name": "Christopher Hesse"},
        {"name": "Mark Chen"},
        {"name": "Eric Sigler"},
        {"name": "Mateusz Litwin"},
        {"name": "Scott Gray"},
        {"name": "Benjamin Chess"},
        {"name": "Jack Clark"},
        {"name": "Christopher Berner"},
        {"name": "Sam McCandlish"},
        {"name": "Alec Radford"},
        {"name": "Ilya Sutskever"},
        {"name": "Dario Amodei"}
      ],
      "year": 2020,
      "url": "https://arxiv.org/abs/2005.14165",
      "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions.",
      "source": "crawler",
      "crawledAt": "2024-11-05T05:43:20.000Z",
      "tags": ["gpt", "language models", "few-shot learning", "nlp"],
      "dateAdded": "2024-11-05T05:43:20.000Z"
    },
    {
      "id": "paper_1730783000003_jkl012mno",
      "title": "Post-Training Quantization for Large Language Models",
      "authors": [
        {"name": "Tim Dettmers"},
        {"name": "Mike Lewis"},
        {"name": "Younes Belkada"},
        {"name": "Luke Zettlemoyer"}
      ],
      "year": 2023,
      "url": "https://arxiv.org/abs/2308.08079",
      "abstract": "This paper introduces a novel post-training quantization method for large language models that achieves high compression rates while maintaining model performance. The method uses a combination of quantization-aware training techniques and optimized weight representations to reduce model size by up to 75% with minimal accuracy loss.",
      "source": "crawler",
      "crawledAt": "2024-11-05T05:43:20.000Z",
      "tags": ["quantization", "large language models", "model compression", "efficiency"],
      "dateAdded": "2024-11-05T05:43:20.000Z"
    },
    {
      "id": "paper_1730783000004_mno345pqr",
      "title": "LoRA: Low-Rank Adaptation of Large Language Models",
      "authors": [
        {"name": "Edward J. Hu"},
        {"name": "Yelong Shen"},
        {"name": "Phillip Wallis"},
        {"name": "Zeyuan Allen-Zhu"},
        {"name": "Yuanzhi Li"},
        {"name": "Shean Wang"},
        {"name": "Lu Wang"},
        {"name": "Weizhu Chen"}
      ],
      "year": 2021,
      "url": "https://arxiv.org/abs/2106.09685",
      "abstract": "An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all of a model's parameters, becomes less feasible. Using GPT-3 as an example, deploying independent instances of fine-tuned models, each with 175 billion parameters, is prohibitively expensive.",
      "source": "crawler",
      "crawledAt": "2024-11-05T05:43:20.000Z",
      "tags": ["lora", "fine-tuning", "parameter efficient", "large language models"],
      "dateAdded": "2024-11-05T05:43:20.000Z"
    }
  ],
  "searchResults": [],
  "chatHistory": [],
  "lastUpdated": "2024-11-05T05:43:20.000Z",
  "version": "1.0"
}